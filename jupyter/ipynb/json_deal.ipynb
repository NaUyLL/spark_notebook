{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@41ac9db6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkConf@41ac9db6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val conf=new SparkConf()\n",
    "      .setMaster(\"local\")//启动本地化计算\n",
    "      .setAppName(\"json\")//设置本程序名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.SparkContext@71f7443\n",
       "spark = org.apache.spark.sql.SparkSession@2811a303\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@2811a303"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val sc=new SparkContext(conf)\n",
    "    val spark = SparkSession.builder().appName(\"json_app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [metaDataInfo: struct<Export time: string, Exported by: string ... 2 more fields>, pageSize: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[metaDataInfo: struct<Export time: string, Exported by: string ... 2 more fields>, pageSize: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val data=spark.read.option(\"multiLine\", true).option(\"mode\", \"PERMISSIVE\").json(\"../data/test.json\")//读取本地文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+-------------+----------+----------+----------+\n",
      "|        metaDataInfo|pageSize|            policies|  queryTimeMS|resultSize|startIndex|totalCount|\n",
      "+--------------------+--------+--------------------+-------------+----------+----------+----------+\n",
      "|[Mar 21, 2018 6:3...|       0|[[[], [], [], [],...|1521614211412|         0|         0|         0|\n",
      "+--------------------+--------+--------------------+-------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(explode(data(\"policies\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            policies|\n",
      "+--------------------+\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "|[[], [], [], [], ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonDF = [policies: struct<allowExceptions: array<string>, dataMaskPolicyItems: array<string> ... 17 more fields>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[policies: struct<allowExceptions: array<string>, dataMaskPolicyItems: array<string> ... 17 more fields>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var jsonDF = data.select(explode(data(\"policies\"))).toDF(\"policies\")\n",
    "jsonDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonDF = [service: string, users: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[service: string, users: array<string>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonDF=jsonDF.select(jsonDF(\"policies.service\"), explode(jsonDF(\"policies.policyItems.users\"))).toDF(\"service\",\"users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|             service|          users|\n",
      "+--------------------+---------------+\n",
      "|Atlas_service_new...|[dhaval, admin]|\n",
      "|Atlas_service_new...|[dhaval, admin]|\n",
      "|Atlas_service_new...|[dhaval, admin]|\n",
      "|        hdfs service|       [dhaval]|\n",
      "|        hdfs service|     [keyadmin]|\n",
      "|     hdfs service 2d|       [dhaval]|\n",
      "|     hdfs service 2d|     [keyadmin]|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             service|   users|\n",
      "+--------------------+--------+\n",
      "|     hdfs service 2d|keyadmin|\n",
      "|Atlas_service_new...|  dhaval|\n",
      "|     hdfs service 2d|  dhaval|\n",
      "|        hdfs service|keyadmin|\n",
      "|Atlas_service_new...|   admin|\n",
      "|        hdfs service|  dhaval|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonDF = [service: string, users: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[service: string, users: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonDF = jsonDF.select(jsonDF(\"service\"), explode(jsonDF(\"users\"))).toDF(\"service\",\"users\").distinct\n",
    "jsonDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
