{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@5b3b8630\n",
       "sc = org.apache.spark.SparkContext@3b2783a6\n",
       "spark = org.apache.spark.sql.SparkSession@471d37ad\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@471d37ad"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf=new SparkConf()\n",
    "      .setMaster(\"local\")//启动本地化计算\n",
    "      .setAppName(\"testRdd\")//设置本程序名称\n",
    "val sc=new SparkContext(conf)\n",
    "val spark = SparkSession.builder().appName(\"csv_app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csv1 = [StartDate: string, EndDate: string ... 3244 more fields]\n",
       "csv2 = [Attribute: string, Electrolux: string ... 51 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Attribute: string, Electrolux: string ... 51 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csv1=spark.read.option(\"header\", \"true\").csv(\"../data/input.csv\")//读取本地文件\n",
    "val csv2=spark.read.option(\"header\", \"true\").csv(\"../data/attribute_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col2 = Array(Attribute, Electrolux, AEG, Westinghouse, Zanussi, Simpson, Haier, Asko, Smeg, LG, Samsung, Fisher & Paykel, Bosch, Miele, Hisense, Kelvinator, Panasonic, Sharp, Toshiba, Hitachi, Daikin, Elba, Aqua, Teka, Midea, Beko, Rinnai, Fotile, Mitsubishi, Modena, Tecnogas, Ariston, Polytron, Sanken, White-Westinghouse, Fujidenzo, La Germania, Whirlpool, Defy, Siemens, KIC, Olympic Electric, Fresh, Kiriazi, White Point, Arthur Martin, Franke, Bauknecht, Sauter, Tornado, Gibson, Maytag, Indesit)\n",
       "brand = Array(Electrolux, AEG, Westinghouse, Zanussi, Simpson, Haier, Asko, Smeg, LG, Samsung, Fisher & Paykel, Bosch, Miele, Hisense, Kelvinator, Panasonic, Sharp, Toshiba, Hitachi, Daikin, Elba, Aqua, Teka, Midea, Beko, Rinnai, Fotile, Mitsubishi, Modena, Tecnog...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Electrolux, AEG, Westinghouse, Zanussi, Simpson, Haier, Asko, Smeg, LG, Samsung, Fisher & Paykel, Bosch, Miele, Hisense, Kelvinator, Panasonic, Sharp, Toshiba, Hitachi, Daikin, Elba, Aqua, Teka, Midea, Beko, Rinnai, Fotile, Mitsubishi, Modena, Tecnog..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val col2 = csv2.columns\n",
    "val brand = col2.filter(!_.contains(\"Attribute\"))\n",
    "val col1 = csv1.columns\n",
    "val Ques = col1.slice(col1.indexWhere(_==\"activities_1\"), col1.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:32: error: overloaded method value select with alternatives:\n  [U1, U2, U3](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1], c2: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U2], c3: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U3])org.apache.spark.sql.Dataset[(U1, U2, U3)] <and>\n  (col: String,cols: String*)org.apache.spark.sql.DataFrame <and>\n  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame\n cannot be applied to (String, String, Any)\n       var csv1DF = csv1.select(\"ResponseId\",\"EndDate\",Ques(0)).toDF(\"ResponseId\",\"EndDate\", \"Value\").withColumn(\"Question\", lit(Ques(0)))\n                         ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "var csv1DF = csv1.select(\"ResponseId\",\"EndDate\",Ques(0)).toDF(\"ResponseId\",\"EndDate\", \"Value\").withColumn(\"Question\", lit(Ques(0)))\n",
    "for(i<-(1 until Ques.length)){\n",
    "    csv1DF = csv1DF.union(\n",
    "    csv1.select(\"ResponseId\",\"EndDate\",Ques(i))\n",
    "        .toDF(\"ResponseId\",\"EndDate\", \"Value\")\n",
    "        .withColumn(\"Question\", lit(Ques(i)))\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:29: error: not found: value csv1DF\n       csv1DF.select(\"Question\").limit(4).show\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csv2DF = [Attribute: string, Question: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Attribute: string, Question: string ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var csv2DF = csv2.select(\"Attribute\",brand(0)).toDF(\"Attribute\", \"Question\").withColumn(\"Brand\", lit(brand(0)))\n",
    "for(i<-(1 until brand.length)){\n",
    "    csv2DF = csv2DF.union(\n",
    "    csv2.select(\"Attribute\",brand(i))\n",
    "        .toDF(\"Attribute\", \"Question\")\n",
    "        .withColumn(\"Brand\", lit(brand(i)))\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ques = Array(sustainability_57, human-centricity_57, style_57, premiumness_57, innovation_57, status_57, modernity_57, eco-friendliness_57, reliability_57, quality_57, intuitiveness_57, value-for-money_57, family-focus_57, simple_57, clever_57, trust_57, love_57, sustainability_58, human-centricity_58, style_58, premiumness_58, innovation_58, status_58, modernity_58, eco-friendliness_58, reliability_58, quality_58, intuitiveness_58, value-for-money_58, family-focus_58, simple_58, clever_58, trust_58, love_58, sustainability_59, human-centricity_59, style_59, premiumness_59, innovation_59, status_59, modernity_59, eco-friendliness_59, reliability_59, quality_59, intuitiveness_59, value-for-money_59, family-focus_59, simple_59, clever_59, trust_59, love_59, sustainability_95, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(sustainability_57, human-centricity_57, style_57, premiumness_57, innovation_57, status_57, modernity_57, eco-friendliness_57, reliability_57, quality_57, intuitiveness_57, value-for-money_57, family-focus_57, simple_57, clever_57, trust_57, love_57, sustainability_58, human-centricity_58, style_58, premiumness_58, innovation_58, status_58, modernity_58, eco-friendliness_58, reliability_58, quality_58, intuitiveness_58, value-for-money_58, family-focus_58, simple_58, clever_58, trust_58, love_58, sustainability_59, human-centricity_59, style_59, premiumness_59, innovation_59, status_59, modernity_59, eco-friendliness_59, reliability_59, quality_59, intuitiveness_59, value-for-money_59, family-focus_59, simple_59, clever_59, trust_59, love_59, sustainability_95, ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Ques = csv2DF.select(\"Question\").collect.map(_(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:32: error: overloaded method value select with alternatives:\n  [U1, U2, U3](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1], c2: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U2], c3: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U3])org.apache.spark.sql.Dataset[(U1, U2, U3)] <and>\n  (col: String,cols: String*)org.apache.spark.sql.DataFrame <and>\n  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame\n cannot be applied to (String, String, Any)\n       var csv1DF = csv1.select(\"ResponseId\",\"EndDate\",Ques(0)).toDF(\"ResponseId\",\"EndDate\", \"Value\").withColumn(\"Question\", lit(Ques(0)))\n                         ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "var csv1DF = csv1.select(\"ResponseId\",\"EndDate\",Ques(0)).toDF(\"ResponseId\",\"EndDate\", \"Value\").withColumn(\"Question\", lit(Ques(0)))\n",
    "for(i<-(1 until Ques.length)){\n",
    "    csv1DF = csv1DF.union(\n",
    "    csv1.select(\"ResponseId\",\"EndDate\",Ques(i))\n",
    "        .toDF(\"ResponseId\",\"EndDate\", \"Value\")\n",
    "        .withColumn(\"Question\", lit(Ques(i)))\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
